{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import imageio\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_images_and_camera_metadata(data_dir, split='train', num_classes=1):\n",
    "    \"\"\"\n",
    "    Load images and camera data from NeRF synthetic dataset format.\n",
    "    Params:\n",
    "    - class_dir: Directory for the dataset \n",
    "    - split: Train, Test, or val\n",
    "    - num_classes: Number of class folders to use\n",
    "    Returns:\n",
    "    - images: Tensor of shape [N, H, W, 3]\n",
    "    - poses: Tensor of shape [N, 4, 4]\n",
    "    - camera_angle_x: Field of view in the x-direction\n",
    "    \"\"\"\n",
    "    all_classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    selected_classes = all_classes[:num_classes]  # Take first N classes\n",
    "    images = []\n",
    "    poses = []\n",
    "    camera_angle_x = None\n",
    "    for cls in selected_classes:\n",
    "        class_dir = os.path.join(data_dir, cls)\n",
    "        json_path = os.path.join(class_dir, f'transforms_{split}.json')\n",
    "        with open(json_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        # Store camera angle (same for all images in class)\n",
    "        if camera_angle_x is None:\n",
    "            camera_angle_x = metadata['camera_angle_x']\n",
    "        elif not math.isclose(camera_angle_x, metadata['camera_angle_x']):\n",
    "            print(f\"Warning: Different camera_angle_x in {cls}\")\n",
    "\n",
    "        for frame in metadata['frames']:\n",
    "            # Handle different path formats (some end with .png, some don't)\n",
    "            file_path = frame['file_path']\n",
    "            if not file_path.endswith('.png'):\n",
    "                file_path += '.png'\n",
    "            # Handle both cases where images are in split folder or not\n",
    "            img_path = os.path.join(class_dir, file_path)\n",
    "            if not os.path.exists(img_path):\n",
    "                # Try looking in the split subdirectory\n",
    "                img_path = os.path.join(class_dir, split, os.path.basename(file_path))\n",
    "            image = imageio.v2.imread(img_path).astype(np.float32) / 255.0\n",
    "            transform_matrix = np.array(frame['transform_matrix'], dtype=np.float32)\n",
    "            images.append(image)\n",
    "            poses.append(transform_matrix)\n",
    "    if not images:\n",
    "        raise RuntimeError(f\"No images found in {data_dir} for split {split}\")\n",
    "    images = torch.tensor(np.array(images))\n",
    "    poses = torch.tensor(np.array(poses))\n",
    "\n",
    "    return (images, poses, camera_angle_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b78990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_camera_rays(camera_pose, H=800, W=800, camera_angle_x=0.6911112070083618):\n",
    "    \"\"\"\n",
    "    Generates the ray origins and directions for all pixels in an image.\n",
    "    Params:\n",
    "    - camera_pose: [4, 4] tensor, the transformation matrix\n",
    "    - H, W: The image height and width\n",
    "    - camera_angle_x: Field of view in the x-direction\n",
    "    Returns:\n",
    "    - rays_o: [H*W, 3] tensor of ray origins (same for all pixels becuase the origin is the camera)\n",
    "    - rays_d: [H*W, 3] tensor of ray directions (world space)\n",
    "    \"\"\"\n",
    "    # get the device (cpu/gpu) the camera pose is on so we compute everything in the same place\n",
    "    device = camera_pose.device \n",
    "    if isinstance(camera_angle_x, float):\n",
    "        camera_angle_x = torch.tensor(camera_angle_x, device=device)\n",
    "    focal = 0.5 * W / torch.tan(camera_angle_x * 0.5)\n",
    "    # Generate the pixel grid coordinates and convert them to camera space directions\n",
    "    i, j = torch.meshgrid(torch.arange(W, dtype=torch.float32, device=device), torch.arange(H, dtype=torch.float32, device=device), indexing='xy')\n",
    "    x = (i - W * 0.5) / focal\n",
    "    y = -(j - H * 0.5) / focal\n",
    "    z = -torch.ones_like(x) \n",
    "    dirs = torch.stack([x, y, z], dim=-1)\n",
    "    # Transform ray directions from camera space to world space then normalize\n",
    "    ray_directions = torch.sum(dirs[..., None, :] * camera_pose[:3, :3], dim=-1)\n",
    "    ray_directions = ray_directions / torch.norm(ray_directions, dim=-1, keepdim=True)\n",
    "    # Get the ray origins from camera position and reshapes the outputs\n",
    "    ray_origins = camera_pose[:3, 3].expand(ray_directions.shape)  \n",
    "    rays_o = ray_origins.reshape(-1, 3)\n",
    "    rays_d = ray_directions.reshape(-1, 3)\n",
    "\n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(inputs, num_freqs):\n",
    "    \"\"\"\n",
    "    Converts raw input positions to higher-dimensional representations using sinusoidal encoding.\n",
    "    Params:\n",
    "    - inputs: Tensor of input positions (e.g., x, y, z coordinates)\n",
    "    - num_freqs: Number of frequency bands to use for encoding\n",
    "    Returns:\n",
    "    - encoded: Higher-dimensional encoding that helps model high-frequency details\n",
    "    \"\"\"\n",
    "    # Generate the sinusoidal encodings for each frequency band\n",
    "    encodings = []\n",
    "    for i in range(num_freqs):\n",
    "        freq = 2 ** i * math.pi\n",
    "        encodings.append(torch.sin(freq * inputs))\n",
    "        encodings.append(torch.cos(freq * inputs))\n",
    "    return torch.cat(encodings, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a49bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRFModel(nn.Module):\n",
    "    \"\"\"\n",
    "    An MLP that represents the neural radiance field (NeRF).\n",
    "    Takes 3D spatial coordinates and viewing directions and returns density and RGB color.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_frequencies, input_pos_dimensions, input_dir_dimensions):\n",
    "        \"\"\"\n",
    "        initialize the NeRF model architecture including hidden layers,\n",
    "        activation functions, and output heads for density and color.\n",
    "        \"\"\"\n",
    "        super(NeRFModel, self).__init__()\n",
    "\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.pos_dimensions = input_pos_dimensions * 2 * num_frequencies\n",
    "        self.dir_dimensions = input_dir_dimensions * 2 * num_frequencies\n",
    "        hidden_size = 256\n",
    "\n",
    "        self.pts_linears = nn.ModuleList([\n",
    "            nn.Linear(self.pos_dimensions, hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),       \n",
    "            nn.Linear(hidden_size, hidden_size),    \n",
    "            nn.Linear(hidden_size, hidden_size),   \n",
    "            #skip connection layer, a bit confusing but tldr we need skip layers for vanishing gradiant problem         \n",
    "            nn.Linear(hidden_size + self.pos_dimensions, hidden_size),   \n",
    "            nn.Linear(hidden_size, hidden_size),                \n",
    "            nn.Linear(hidden_size, hidden_size),          \n",
    "            nn.Linear(hidden_size, hidden_size)      \n",
    "        ])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_sigma = nn.Linear(hidden_size, 1)\n",
    "        self.fc_feature = nn.Linear(hidden_size, hidden_size)\n",
    "        self.view_linear = nn.Linear(hidden_size + self.dir_dimensions, 128)\n",
    "        self.fc_rgb = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, positions, view_directions):\n",
    "        \"\"\"\n",
    "        Compute density and color values from given 3D positions and view directions.\n",
    "        \"\"\"\n",
    "        pos_encoded = positional_encoding(positions, self.num_frequencies)\n",
    "        dir_encoded = positional_encoding(view_directions, self.num_frequencies)\n",
    "        h = pos_encoded\n",
    "        for i, layer in enumerate(self.pts_linears):\n",
    "            if i == 4: #for skip connection layer\n",
    "                h = torch.cat([h, pos_encoded], dim=-1)\n",
    "            h = self.relu(layer(h))\n",
    "        sigma = self.fc_sigma(h)\n",
    "        features = self.relu(self.fc_feature(h)) \n",
    "        h_dir = self.relu(self.view_linear(torch.cat([features, dir_encoded], dim=-1)))\n",
    "        rgb = torch.sigmoid(self.fc_rgb(h_dir))\n",
    "        output = torch.cat([sigma, rgb], dim=-1)  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23231b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(bins, weights, N_samples, device):\n",
    "    \"\"\"\n",
    "    Hierarchical sampling using inverse transform sampling.\n",
    "    Params:\n",
    "    - bins: [N_rays, N_samples-1], bin edges\n",
    "    - weights: [N_rays, N_samples-1], weight for each bin\n",
    "    - N_samples: The number of samples to draw\n",
    "    - device: The device we are using\n",
    "    Returns:\n",
    "    - samples: [N_rays, N_samples] newly sampled depths\n",
    "    \"\"\"\n",
    "    # Calculates the PDF and CDF from the weights\n",
    "    weights += 1e-5  # Avoid nans\n",
    "    pdf = weights / torch.sum(weights, -1, keepdim=True)\n",
    "    cdf = torch.cumsum(pdf, -1)\n",
    "    cdf = torch.cat([torch.zeros_like(cdf[:, :1]), cdf], -1)\n",
    "    u = torch.rand(cdf.shape[0], N_samples, device=device)\n",
    "    # Find locations of samples through inverse CDF\n",
    "    inds = torch.searchsorted(cdf, u, right=True)\n",
    "    below = torch.clamp(inds - 1, min=0)\n",
    "    above = torch.clamp(inds, max=cdf.shape[-1] - 1)\n",
    "    # Gets the CDF and bin values for interpolation\n",
    "    inds_g = torch.stack([below, above], -1)\n",
    "    cdf_g = torch.gather(cdf.unsqueeze(1).expand(-1, N_samples, -1), 2, inds_g)\n",
    "    bins_g = torch.gather(bins.unsqueeze(1).expand(-1, N_samples, -1), 2, inds_g)\n",
    "    # Do linear interpolation to get final samples\n",
    "    denom = cdf_g[..., 1] - cdf_g[..., 0]\n",
    "    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n",
    "    t = (u - cdf_g[..., 0]) / denom\n",
    "    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(model, ray_origins, ray_directions, N_coarse=64, N_fine=128, near=2.0, far=10.0, device = 'cpu'):\n",
    "    \"\"\"\n",
    "    Render rays using NeRF with hierarchical sampling.\n",
    "    Params:\n",
    "    - model: Our NeRf model\n",
    "    - ray_origins: Tensor of ray origin positions\n",
    "    - ray_directions: Tensor of ray direction vectors\n",
    "    - N_coarse: Number of samples for coarse sampling\n",
    "    - N_fine: Number of samples for fine sampling\n",
    "    - near: Near bound for sampling\n",
    "    - far: Far bound for sampling\n",
    "    - device: Device being used\n",
    "    Returns:\n",
    "    - rgb: Tensor of rendered colors\n",
    "    \"\"\"\n",
    "    N_rays = ray_origins.shape[0]\n",
    "    # Coarse sampling\n",
    "    z_vals_coarse = torch.linspace(near, far, N_coarse, device=device).expand(N_rays, N_coarse)\n",
    "    pts_coarse = ray_origins.unsqueeze(1) + ray_directions.unsqueeze(1) * z_vals_coarse.unsqueeze(-1)\n",
    "    pts_coarse_flat = pts_coarse.reshape(-1, 3)\n",
    "    dirs_coarse_flat = ray_directions.unsqueeze(1).expand(-1, N_coarse, -1).reshape(-1, 3)\n",
    "    with torch.no_grad():  # Reduce memory during coarse pass\n",
    "        outputs_coarse = model(pts_coarse_flat, dirs_coarse_flat)\n",
    "    sigma_coarse = outputs_coarse[:, 0].reshape(N_rays, N_coarse)\n",
    "    rgb_coarse = outputs_coarse[:, 1:].reshape(N_rays, N_coarse, 3)\n",
    "    # Alpha compositing\n",
    "    deltas = z_vals_coarse[:, 1:] - z_vals_coarse[:, :-1]\n",
    "    delta_inf = torch.full((N_rays, 1), 1e10, device=device)\n",
    "    deltas = torch.cat([deltas, delta_inf], dim=1)\n",
    "    alpha = 1 - torch.exp(-sigma_coarse * deltas)\n",
    "    trans = torch.cumprod(1 - alpha + 1e-10, dim=1)[:, :-1]\n",
    "    weights_coarse = alpha * torch.cat([torch.ones_like(trans[:, :1]), trans], dim=1)\n",
    "    # Fine sampling\n",
    "    z_vals_mid = 0.5 * (z_vals_coarse[:, 1:] + z_vals_coarse[:, :-1])\n",
    "    z_vals_fine = sample_pdf(z_vals_mid, weights_coarse[:, 1:-1], N_fine, device=device)\n",
    "    z_vals_combined, _ = torch.sort(torch.cat([z_vals_coarse, z_vals_fine], dim=-1), dim=-1)\n",
    "    pts_fine = ray_origins.unsqueeze(1) + ray_directions.unsqueeze(1) * z_vals_combined.unsqueeze(-1)\n",
    "    pts_fine_flat = pts_fine.reshape(-1, 3)\n",
    "    dirs_fine_flat = ray_directions.unsqueeze(1).expand(-1, N_coarse + N_fine, -1).reshape(-1, 3)\n",
    "    outputs_fine = model(pts_fine_flat, dirs_fine_flat)\n",
    "    sigma_fine = outputs_fine[:, 0].reshape(N_rays, N_coarse + N_fine)\n",
    "    rgb_fine = outputs_fine[:, 1:].reshape(N_rays, N_coarse + N_fine, 3)\n",
    "    # Final rendering\n",
    "    deltas_fine = z_vals_combined[:, 1:] - z_vals_combined[:, :-1]\n",
    "    deltas_fine = torch.cat([deltas_fine, delta_inf], dim=1)\n",
    "    alpha_fine = 1 - torch.exp(-sigma_fine * deltas_fine)\n",
    "    trans_fine = torch.cumprod(1 - alpha_fine + 1e-10, dim=1)[:, :-1]\n",
    "    weights_fine = alpha_fine * torch.cat([torch.ones_like(trans_fine[:, :1]), trans_fine], dim=1)\n",
    "    rgb = torch.sum(weights_fine.unsqueeze(-1) * rgb_fine, dim=1)\n",
    "\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad39d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nerf(model, training_rays, training_colors, epochs, batch_size, learning_rate, device):\n",
    "    \"\"\"\n",
    "    Trains the NeRF model.\n",
    "    Params:\n",
    "    - model: The NeRF model to train\n",
    "    - training_rays: ray_origins & ray_directions\n",
    "    - training_colors: Ground truth RGB colors for each ray\n",
    "    - epochs: Number of training epochs\n",
    "    - batch_size: Number of rays per batch\n",
    "    - learning_rate: Learning rate for the optimizer\n",
    "    - device: Device being used\n",
    "    Returns:\n",
    "    - model: Trained NeRF model\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    rays_o, rays_d = training_rays\n",
    "    dataset = torch.utils.data.TensorDataset(rays_o, rays_d, training_colors)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for rays_o_b, rays_d_b, colors_b in loader:\n",
    "            rays_o_b = rays_o_b.to(device)\n",
    "            rays_d_b = rays_d_b.to(device)\n",
    "            colors_b = colors_b.to(device)\n",
    "            preds = render_rays(model, rays_o_b, rays_d_b, device = device) # all on CPU\n",
    "            loss = nn.functional.mse_loss(preds, colors_b)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * rays_o_b.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_novel_views(model, output_dir, num_frames=30, resolution=(400, 400)):\n",
    "    \"\"\"\n",
    "    Render frames of the scene from new camera viewpoints in a circular trajectory.\n",
    "    Params:\n",
    "    - model: Trained NeRF model\n",
    "    - output_dir: Directory to save rendered frames\n",
    "    - num_frames: Number of frames to render\n",
    "    - resolution: Resolution for output images\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    # Renders the frames from different viewpoints along a circular path\n",
    "    for i in range(num_frames):\n",
    "        theta = 2 * math.pi * i / num_frames\n",
    "        camera_pose = torch.tensor([\n",
    "            [np.cos(theta), -np.sin(theta), 0, 4 * np.cos(theta)],\n",
    "            [np.sin(theta),  np.cos(theta), 0, 4 * np.sin(theta)],\n",
    "            [0,             0,             1, 2],\n",
    "            [0,             0,             0, 1]\n",
    "        ], dtype=torch.float32)\n",
    "        rays_o, rays_d = generate_camera_rays(camera_pose, H=resolution[0], W=resolution[1])\n",
    "        with torch.no_grad():\n",
    "            rgb = render_rays(model, rays_o, rays_d, near=2.0, far=6.0, device='cpu')\n",
    "        img = rgb.reshape(resolution[0], resolution[1], 3).numpy()\n",
    "        imageio.imwrite(os.path.join(output_dir, f\"frame_{i:03d}.png\"), (img * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(data_dir, output_dir):\n",
    "    \"\"\"\n",
    "    NeRF pipeline.\n",
    "    Params:\n",
    "    - data_dir: Directory containing the input images and camera poses\n",
    "    - output_dir: Directory to save output renderings\n",
    "    \"\"\"\n",
    "    device_cpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data_dir = \"data/nerf_synthetic\"\n",
    "    if data_dir is None:\n",
    "        data_dir = \"data/nerf_synthetic\"\n",
    "    # Load and prepare image and camera data\n",
    "    images, poses, camera_angle_x = load_synthetic_images_and_camera_metadata(data_dir)\n",
    "    N, H, W, _ = images.shape\n",
    "    if isinstance(camera_angle_x, float):\n",
    "        # Single class case - use same angle for all images\n",
    "        camera_angle_x = [camera_angle_x] * N\n",
    "    elif isinstance(camera_angle_x, list):\n",
    "        # Multi-class case - ensure we have enough angles\n",
    "        if len(camera_angle_x) < N:\n",
    "            camera_angle_x = camera_angle_x * (N // len(camera_angle_x) + 1)\n",
    "            camera_angle_x = camera_angle_x[:N]\n",
    "    # Generate rays and collect RGB values for all images\n",
    "    all_rays_o = []\n",
    "    all_rays_d = []\n",
    "    all_rgb = []\n",
    "    for i in range(N):\n",
    "        angle_x = camera_angle_x[0] if isinstance(camera_angle_x, list) else camera_angle_x\n",
    "        rays_o, rays_d = generate_camera_rays(poses[i], H=H, W=W, camera_angle_x=angle_x)\n",
    "        img = images[i]\n",
    "        if img.shape[2] > 3: # If RGBA, remove alpha channel\n",
    "            img = img[..., :3]\n",
    "        elif img.shape[2] == 1: # If grayscale, convert to RGB\n",
    "            img = img.expand(-1, -1, 3)\n",
    "        expected_size = H * W * 3\n",
    "        if img.numel() != expected_size:\n",
    "            print(f\"Warning: Image {i} has unexpected size {img.shape}, resizing\")\n",
    "            img = img.view(3, H, W).permute(1, 2, 0)\n",
    "        all_rays_o.append(rays_o)\n",
    "        all_rays_d.append(rays_d)\n",
    "        all_rgb.append(img.reshape(H * W, 3))  \n",
    "    # Combine data from all images\n",
    "    rays_o = torch.cat(all_rays_o, dim=0) \n",
    "    rays_d = torch.cat(all_rays_d, dim=0)\n",
    "    colors = torch.cat(all_rgb, dim=0)\n",
    "    # Create the NeRF model\n",
    "    pos_input_dim = 3\n",
    "    dir_input_dim = 3\n",
    "    num_frequencies = 10\n",
    "    model = NeRFModel(input_pos_dimensions = pos_input_dim,\n",
    "                      input_dir_dimensions = dir_input_dim,\n",
    "                      num_frequencies = num_frequencies)\n",
    "    model.to(device_cpu)\n",
    "    # Train the NeRF model\n",
    "    train_nerf(\n",
    "        model = model, \n",
    "        training_rays = (rays_o, rays_d),\n",
    "        training_colors = colors,\n",
    "        epochs = 20, \n",
    "        batch_size = 1024, \n",
    "        learning_rate= 5e-4,\n",
    "        device = device_cpu)\n",
    "    render_novel_views(model, \"output_frames\")\n",
    "\n",
    "    print(\"Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_pipeline(\"datadir\", \"outdir\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI1430-FinalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
