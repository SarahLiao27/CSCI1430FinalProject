{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262abce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (4.67.1)\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /users/haabdulr/.local/lib/python3.9/site-packages\n",
      "sysconfig: /users/haabdulr/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: fsspec in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: filelock in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: triton==3.3.0 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: jinja2 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: networkx in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3.9/site-packages (from triton==3.3.0->torch) (53.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /users/haabdulr/.local/lib/python3.9/site-packages\n",
      "sysconfig: /users/haabdulr/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /oscar/home/haabdulr/.local/lib/python3.9/site-packages (2.0.2)\n",
      "\u001b[33mWARNING: Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "distutils: /users/haabdulr/.local/lib/python3.9/site-packages\n",
      "sysconfig: /users/haabdulr/.local/lib64/python3.9/site-packages\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = True\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall torch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall numpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/__init__.py:2240\u001b[0m\n\u001b[1;32m   2236\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, classes)\n\u001b[1;32m   2238\u001b[0m \u001b[38;5;66;03m# quantization depends on torch.fx and torch.ops\u001b[39;00m\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;66;03m# Import quantization\u001b[39;00m\n\u001b[0;32m-> 2240\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization \u001b[38;5;28;01mas\u001b[39;00m quantization  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m   2242\u001b[0m \u001b[38;5;66;03m# Import the quasi random sampler\u001b[39;00m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quasirandom \u001b[38;5;28;01mas\u001b[39;00m quasirandom  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/quantization/__init__.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuser_method_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/quantization/qconfig.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flake8: noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file is in the process of migration to `torch/ao/quantization`, and\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mis kept here for compatibility while the migration process is ongoing.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mhere.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     _add_module_to_qconfig_obs_ctr,\n\u001b[1;32m     11\u001b[0m     _assert_valid_qconfig,\n\u001b[1;32m     12\u001b[0m     default_activation_only_qconfig,\n\u001b[1;32m     13\u001b[0m     default_debug_qconfig,\n\u001b[1;32m     14\u001b[0m     default_dynamic_qconfig,\n\u001b[1;32m     15\u001b[0m     default_per_channel_qconfig,\n\u001b[1;32m     16\u001b[0m     default_qat_qconfig,\n\u001b[1;32m     17\u001b[0m     default_qat_qconfig_v2,\n\u001b[1;32m     18\u001b[0m     default_qconfig,\n\u001b[1;32m     19\u001b[0m     default_weight_only_qconfig,\n\u001b[1;32m     20\u001b[0m     float16_dynamic_qconfig,\n\u001b[1;32m     21\u001b[0m     float16_static_qconfig,\n\u001b[1;32m     22\u001b[0m     float_qparams_weight_only_qconfig,\n\u001b[1;32m     23\u001b[0m     get_default_qat_qconfig,\n\u001b[1;32m     24\u001b[0m     get_default_qconfig,\n\u001b[1;32m     25\u001b[0m     per_channel_dynamic_qconfig,\n\u001b[1;32m     26\u001b[0m     QConfig,\n\u001b[1;32m     27\u001b[0m     qconfig_equals,\n\u001b[1;32m     28\u001b[0m     QConfigAny,\n\u001b[1;32m     29\u001b[0m     QConfigDynamic,\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/ao/quantization/__init__.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfuser_method_mappings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobserver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_numeric_debugger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     compare_results,\n\u001b[1;32m     14\u001b[0m     CUSTOM_KEY,\n\u001b[1;32m     15\u001b[0m     extract_results_from_loggers,\n\u001b[1;32m     16\u001b[0m     generate_numeric_debug_handle,\n\u001b[1;32m     17\u001b[0m     NUMERIC_DEBUG_HANDLE_KEY,\n\u001b[1;32m     18\u001b[0m     prepare_for_propagation_comparison,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     _allow_exported_model_train_eval \u001b[38;5;28;01mas\u001b[39;00m allow_exported_model_train_eval,\n\u001b[1;32m     22\u001b[0m     _move_exported_model_to_eval \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_eval,\n\u001b[1;32m     23\u001b[0m     _move_exported_model_to_train \u001b[38;5;28;01mas\u001b[39;00m move_exported_model_to_train,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/ao/quantization/pt2e/_numeric_debugger.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mns\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute_sqnr\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpt2e\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m bfs_trace_with_node_process\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphModule, Node\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/ao/quantization/pt2e/graph_utils.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Optional, Union\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource_matcher_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     check_subgraphs_connected,\n\u001b[1;32m     13\u001b[0m     get_source_partitions,\n\u001b[1;32m     14\u001b[0m     SourcePartition,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/export/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytree\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpass_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PassResult\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpass_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PassManager\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileLike\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/fx/passes/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     graph_drawer,\n\u001b[1;32m      3\u001b[0m     graph_manipulation,\n\u001b[1;32m      4\u001b[0m     net_min_base,\n\u001b[1;32m      5\u001b[0m     operator_support,\n\u001b[1;32m      6\u001b[0m     param_fetch,\n\u001b[1;32m      7\u001b[0m     reinplace,\n\u001b[1;32m      8\u001b[0m     runtime_assert,\n\u001b[1;32m      9\u001b[0m     shape_prop,\n\u001b[1;32m     10\u001b[0m     split_module,\n\u001b[1;32m     11\u001b[0m     split_utils,\n\u001b[1;32m     12\u001b[0m     splitter_base,\n\u001b[1;32m     13\u001b[0m     tools_common,\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/fx/passes/graph_drawer.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _format_arg, _get_qualified_name\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperator_schemas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_function\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpasses\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape_prop\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorMetadata\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydot\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/fx/passes/shape_prop.py:85\u001b[0m\n\u001b[1;32m     77\u001b[0m             qparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mq_per_channel_axis()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TensorMetadata(\n\u001b[1;32m     80\u001b[0m         shape, dtype, requires_grad, stride, memory_format, is_quantized, qparams\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mShapeProp\u001b[39;00m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx\u001b[49m\u001b[38;5;241m.\u001b[39mInterpreter):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Execute an FX graph Node-by-Node and\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    record the shape and type of the result\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm, fake_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_images_and_camera_metadata(class_dir, num_classes=8):\n",
    "    \"\"\"\n",
    "    Load images, poses, and camera angle from the synthetic dataset of choice.\n",
    "\n",
    "    Param:\n",
    "    - class_dir: Directory for the dataset    \n",
    "    Returns:\n",
    "    - images: Tensor of shape [N, H, W, 3]\n",
    "    - poses: Tensor of shape [N, 4, 4]\n",
    "    - camera_angle_x: Field of view in the x-direction\n",
    "    \"\"\"\n",
    "    # Gets the path\n",
    "    json_path = os.path.join(class_dir, \"transforms_train.json\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Loads images and poses into tensors\n",
    "    images, poses = [], []\n",
    "    for f in data['frames']:\n",
    "        img_file = os.path.join(class_dir, f['file_path'] + '.png')\n",
    "        raw = imageio.v2.imread(img_file)\n",
    "        if raw.ndim == 3 and raw.shape[2] == 4:\n",
    "            raw = raw[..., :3]\n",
    "        elif raw.ndim == 2:\n",
    "            raw = np.stack([raw, raw, raw], axis=-1)\n",
    "        img = raw.astype(np.float32) / 255.0\n",
    "        images.append(img)\n",
    "        poses.append(np.array(f['transform_matrix'], dtype=np.float32))\n",
    "    images = torch.tensor(np.stack(images), dtype=torch.float32)\n",
    "    poses = torch.tensor(np.stack(poses), dtype=torch.float32)\n",
    "    cam_ang = float(data['camera_angle_x'])\n",
    "    N, H, W, _ = images.shape\n",
    "    print(f\"Loaded dataset: images: {images.shape}, poses: {poses.shape}, camera_angle_x: {cam_ang:.4f}\")\n",
    "\n",
    "    # Plot example for comparsion\n",
    "    test_img = images[N-1]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(test_img.numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Example image\")\n",
    "    plt.show()\n",
    "    \n",
    "    return images, poses, cam_ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSplattingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    3D Gaussian Splatting model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_gaussians=5000, device='cpu'):\n",
    "        super(GaussianSplattingModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_gaussians = num_gaussians\n",
    "        # Gaussian parameters: positions, scale, rotation, colors, opacity\n",
    "        # The xyz coordinates of each Gaussian\n",
    "        self.means = nn.Parameter(torch.randn(num_gaussians, 3, device=device) * 2.0)\n",
    "        # The scales controls the size of each Gaussian\n",
    "        self.scales = nn.Parameter(torch.ones(num_gaussians, 3, device=device) * 0.1)\n",
    "        # Rotation represented as quaternions for 3D rotation(common practice for most rendering models)\n",
    "        self.rotations = nn.Parameter(torch.zeros(num_gaussians, 4, device=device))\n",
    "        self.rotations.data[:, 0] = 1.0  # w component = 1 for identity quaternion\n",
    "        # Random RBG values for each Gaussian for now\n",
    "        self.colors = nn.Parameter(torch.rand(num_gaussians, 3, device=device))\n",
    "        # Start with small values for opacities so we can see through them initally\n",
    "        self.opacities = nn.Parameter(torch.ones(num_gaussians, 1, device=device) * 0.01)\n",
    "    \n",
    "    def quaternion_to_rotation_matrix(self, q):\n",
    "        \"\"\"Turns our quaternions to rotation matrices\"\"\"\n",
    "        q = F.normalize(q, dim=1)\n",
    "        w, x, y, z = q[:, 0], q[:, 1], q[:, 2], q[:, 3]\n",
    "        R = torch.zeros(q.shape[0], 3, 3, device=q.device)\n",
    "        # Standard quaternion to matrix formula\n",
    "        R[:, 0, 0] = 1 - 2 * (y**2 + z**2)\n",
    "        R[:, 0, 1] = 2 * (x * y - w * z)\n",
    "        R[:, 0, 2] = 2 * (x * z + w * y)\n",
    "        R[:, 1, 0] = 2 * (x * y + w * z)\n",
    "        R[:, 1, 1] = 1 - 2 * (x**2 + z**2)\n",
    "        R[:, 1, 2] = 2 * (y * z - w * x)\n",
    "        R[:, 2, 0] = 2 * (x * z - w * y)\n",
    "        R[:, 2, 1] = 2 * (y * z + w * x)\n",
    "        R[:, 2, 2] = 1 - 2 * (x**2 + y**2)\n",
    "        \n",
    "        return R\n",
    "    \n",
    "    def compute_covariance_matrices(self):\n",
    "        \"\"\"Compute covariance matrices for all Gaussians--what is used to \"splat\" each Gaussian into the image.\"\"\"\n",
    "        rotations = self.quaternion_to_rotation_matrix(self.rotations)\n",
    "        scales = torch.diag_embed(self.scales**2)\n",
    "        # For each Gaussian: rotation @ scale @ rotation.transpose or R * S * R^T\n",
    "        covariances = torch.bmm(torch.bmm(rotations, scales), rotations.transpose(1, 2))\n",
    "        \n",
    "        return covariances\n",
    "    \n",
    "    def project_gaussians_to_image(self, camera_pose, H, W, focal):\n",
    "        \"\"\"\n",
    "        Splat the 3D Gaussians onto the image plane.\n",
    "        \n",
    "        Param:\n",
    "        - camera_pose: [4, 4], the camera transformation matrix\n",
    "        - H, W: Image height and width\n",
    "        - focal: Focal length\n",
    "        Returns:\n",
    "        - centers_2d: [num_gaussians, 2], the 2D centers of our projected Gaussians\n",
    "        - covs_2d: [num_gaussians, 2, 2], the 2D covariance matrices of projections\n",
    "        - depths: [num_gaussians] the depths of Gaussians from camera\n",
    "        \"\"\"\n",
    "        # Transform Gaussian means into camera space\n",
    "        R = camera_pose[:3, :3] # Camera rotation\n",
    "        t = camera_pose[:3, 3] # Camera translation\n",
    "        means_camera = torch.matmul(self.means, R.t()) + t\n",
    "        depths = means_camera[:, 2]\n",
    "        valid_mask = depths > 0 # Only keep points in front\n",
    "\n",
    "        # Project 3D centers to 2D image coordinates\n",
    "        centers_2d = torch.zeros(self.num_gaussians, 2, device=self.device)\n",
    "        centers_2d[:, 0] = focal * means_camera[:, 0] / torch.clamp(means_camera[:, 2], min=1e-10) + W / 2\n",
    "        centers_2d[:, 1] = focal * means_camera[:, 1] / torch.clamp(means_camera[:, 2], min=1e-10) + H / 2\n",
    "\n",
    "        # Compute the 3D covariance matrices and project their covariance ellipses to 2D\n",
    "        covs_3d = self.compute_covariance_matrices()\n",
    "        covs_camera = torch.bmm(torch.bmm(R.expand(self.num_gaussians, 3, 3), covs_3d), R.expand(self.num_gaussians, 3, 3).transpose(1, 2))\n",
    "        J = torch.zeros(self.num_gaussians, 2, 3, device=self.device) # Jacobian matrix (helpful to maintain Gaussian's ellipsoidal shapes )\n",
    "        Z = torch.clamp(means_camera[:, 2], min=1e-10)\n",
    "        J[:, 0, 0] = focal / Z\n",
    "        J[:, 0, 2] = -focal * means_camera[:, 0] / (Z * Z)\n",
    "        J[:, 1, 1] = focal / Z\n",
    "        J[:, 1, 2] = -focal * means_camera[:, 1] / (Z * Z)\n",
    "        covs_2d = torch.bmm(torch.bmm(J, covs_camera), J.transpose(1, 2))\n",
    "        covs_2d = covs_2d + torch.eye(2, device=self.device).unsqueeze(0) * 1e-6 # To avoid division by 0\n",
    "        \n",
    "        return centers_2d, covs_2d, depths, valid_mask\n",
    "    \n",
    "    def render_image(self, camera_pose, H, W, focal, bg_color=None):\n",
    "        \"\"\"\n",
    "        Render an image by splatting all our Gaussians onto the image plane.\n",
    "        \n",
    "        Params:\n",
    "        - camera_pose: [4, 4], the camera transformation matrix\n",
    "        - H, W: Image height and width\n",
    "        - focal: Focal length\n",
    "        - bg_color: Background color(set to black)\n",
    "        Returns:\n",
    "        - rendered_image: [H, W, 3], rendered image\n",
    "        \"\"\"\n",
    "        if bg_color is None:\n",
    "            bg_color = torch.zeros(3, device=self.device) # Default black\n",
    "        else:\n",
    "            bg_color = torch.tensor(bg_color, device=self.device)\n",
    "        # Get each gaussian blob's 2d center, covariance, depth and a mask that tells us which are in front of the camera\n",
    "        centers_2d, covs_2d, depths, valid_mask = self.project_gaussians_to_image(camera_pose, H, W, focal)\n",
    "        rendered_image = torch.zeros(H, W, 3, device=self.device)\n",
    "        alpha_acc = torch.zeros(H, W, 1, device=self.device)\n",
    "        # Sort Gaussians by depth and remove those behind the camera\n",
    "        valid_indices = torch.where(valid_mask)[0]\n",
    "        if len(valid_indices) == 0:\n",
    "            return torch.ones(H, W, 3, device=self.device) * bg_color.unsqueeze(0).unsqueeze(0)\n",
    "        sorted_indices = valid_indices[torch.argsort(depths[valid_indices], descending=True)]\n",
    "        \n",
    "        # Iterate through sorted Gaussians to determine which pixels it affects and blend its color+opacity into the image.\n",
    "        for idx in sorted_indices:\n",
    "            center = centers_2d[idx]\n",
    "            cov = covs_2d[idx]\n",
    "            color = self.colors[idx]\n",
    "            opacity = torch.sigmoid(self.opacities[idx, 0])\n",
    "            # Skip Gaussians skip if far outside the image\n",
    "            if (center[0] < -W/2 or center[0] > W*1.5 or \n",
    "                center[1] < -H/2 or center[1] > H*1.5):\n",
    "                continue\n",
    "            # Use a bounding box for speed and efficiency(we really need it lol)\n",
    "            std_dev = torch.sqrt(torch.diagonal(cov) + 1e-10)\n",
    "            radius = 3 * torch.max(std_dev).item()\n",
    "            # Compute bounding box\n",
    "            min_x = max(0, int(center[0] - radius))\n",
    "            max_x = min(W, int(center[0] + radius + 1))\n",
    "            min_y = max(0, int(center[1] - radius))\n",
    "            max_y = min(H, int(center[1] + radius + 1))\n",
    "            if min_x >= max_x or min_y >= max_y: # Skip if bounding box is empty\n",
    "                continue\n",
    "            y_coords, x_coords = torch.meshgrid( torch.arange(min_y, max_y, device=self.device), torch.arange(min_x, max_x, device=self.device), indexing='ij')\n",
    "            box_coords = torch.stack([x_coords.flatten(), y_coords.flatten()], dim=1)\n",
    "            diff = box_coords - center.unsqueeze(0)\n",
    "            try: # we need this in the case the covariance doesn't have a inverse (singular) \n",
    "                inv_cov = torch.inverse(cov)\n",
    "            except:\n",
    "                inv_cov = torch.diag(1.0 / (torch.diagonal(cov) + 1e-10))\n",
    "            exponent = -0.5 * torch.sum(torch.matmul(diff, inv_cov) * diff, dim=1)\n",
    "            gauss_val = torch.exp(exponent) * opacity\n",
    "            gauss_val = gauss_val.reshape(max_y - min_y, max_x - min_x, 1) # For broadcasting\n",
    "            color_contribution = color.unsqueeze(0).unsqueeze(0) * gauss_val\n",
    "            # We use Alpha‑blending here it lets us layer semi‑transparent Gaussians correctly\n",
    "            current_alpha = 1.0 - alpha_acc[min_y:max_y, min_x:max_x]\n",
    "            alpha_acc[min_y:max_y, min_x:max_x] = alpha_acc[min_y:max_y, min_x:max_x] + gauss_val * current_alpha\n",
    "            rendered_image[min_y:max_y, min_x:max_x] = (rendered_image[min_y:max_y, min_x:max_x] + color_contribution * current_alpha)\n",
    "        \n",
    "        # For pixels that may be partially or not covered at all we cover it with the background\n",
    "        rendered_image = rendered_image + bg_color.unsqueeze(0).unsqueeze(0) * (1.0 - alpha_acc)\n",
    "        return rendered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_pose_from_angle(theta, radius=4.0, height=2.0):\n",
    "    \"\"\"Create a camera pose matrix from an angle around the y-axis.\"\"\"\n",
    "    # Camera's world position\n",
    "    camera_position = np.array([radius * np.cos(theta), height, radius * np.sin(theta)])\n",
    "    # Aim the camera toward the origin\n",
    "    forward = -camera_position / np.linalg.norm(camera_position)\n",
    "    up = np.array([0, 1, 0]) # Initial \"up\" direction\n",
    "    # Camera's right vector \n",
    "    right = np.cross(up, forward)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    up = np.cross(forward, right)\n",
    "    rotation = np.stack([right, up, forward], axis=1)\n",
    "    # Construct 4x4 transformation matrix\n",
    "    pose = np.eye(4)\n",
    "    pose[:3, :3] = rotation\n",
    "    pose[:3, 3] = camera_position\n",
    "    \n",
    "    return torch.tensor(pose, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gaussian_splatting(model, images, poses, camera_angle_x, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Trains the Gaussian Splatting model.\n",
    "    \n",
    "    Params:\n",
    "    - model: GaussianSplattingModel\n",
    "    - images: [N, H, W, 3], the training images\n",
    "    - poses: [N, 4, 4], the camera poses\n",
    "    - camera_angle_x: Field of view in x-direction\n",
    "    - epochs: Number of training epochs\n",
    "    - lr: Learning rate\n",
    "    \"\"\"\n",
    "    N, H, W, _ = images.shape\n",
    "    focal = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.means, 'lr': lr},\n",
    "        {'params': model.scales, 'lr': lr},\n",
    "        {'params': model.rotations, 'lr': lr * 0.1},  # Lower learning rate for rotations\n",
    "        {'params': model.colors, 'lr': lr},\n",
    "        {'params': model.opacities, 'lr': lr * 0.1}  # Lower learning rate for opacities\n",
    "    ])\n",
    "    indices = torch.randperm(N)\n",
    "    losses = []  \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        indices = torch.randperm(N)\n",
    "        # Process each image\n",
    "        for i in tqdm(range(N), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            idx = indices[i].item()\n",
    "            target_image = images[idx].to(model.device)\n",
    "            pose = poses[idx].to(model.device)\n",
    "            rendered_image = model.render_image(pose, H, W, focal)\n",
    "            loss = F.mse_loss(rendered_image, target_image)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # To keep the scales positive\n",
    "            with torch.no_grad():\n",
    "                model.scales.data.clamp_(min=0.001, max=1.0)\n",
    "                model.rotations.data = F.normalize(model.rotations.data, dim=1) # Normalize quaternions\n",
    "                model.colors.data.clamp_(0, 1) # Clamp colors to [0, 1]\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / N\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        # Show the splat progress every 5 epochs\n",
    "        save_dir = \"generated_images10k400epochs\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0 or epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                test_pose = poses[0].to(model.device)\n",
    "                test_render = model.render_image(test_pose, H, W, focal)\n",
    "                \n",
    "                # Create a figure and save the image\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(images[0].cpu().numpy())\n",
    "                plt.title(\"Target Image\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(test_render.cpu().numpy())\n",
    "                plt.title(f\"Rendered Image (Epoch {epoch+1})\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Save the figure to the specified directory\n",
    "                save_path = os.path.join(save_dir, f\"epoch_{epoch+1}.png\")\n",
    "                plt.savefig(save_path)\n",
    "                plt.close()  # Close the plot to free memory1\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_novel_views(model, output_dir, H, W, focal, num_frames=30):\n",
    "    \"\"\"\n",
    "    Render novel views by moving the camera around the scene.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained GaussianSplattingModel\n",
    "    - output_dir: A directory to put the views\n",
    "    - H, W: Image height and width\n",
    "    - focal: Focal length\n",
    "    - num_frames: Number of frames we want to render\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_frames):\n",
    "            # Create camera pose--rotates around the scene\n",
    "            theta = 2 * math.pi * i / num_frames\n",
    "            camera_pose = camera_pose_from_angle(theta)\n",
    "            rendered_image = model.render_image(camera_pose.to(model.device), H, W, focal)\n",
    "            img_np = rendered_image.cpu().numpy()\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "            imageio.imwrite(os.path.join(output_dir, f\"frame_{i:03d}.png\"), img_uint8)\n",
    "            # Display progress\n",
    "            if i % 5 == 0:\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(img_np)\n",
    "                plt.title(f\"Frame {i}/{num_frames}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "    \n",
    "    print(f\"Rendered {num_frames} frames to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29bca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(data_dir=\"data/nerf_synthetic/hotdog\", output_dir=\"gaussian_output1\"): # TODO CHANGE THIS\n",
    "    \"\"\"\n",
    "    Main pipeline for Gaussian Splatting:\n",
    "    1. Load data\n",
    "    2. Create model\n",
    "    3. Train model\n",
    "    4. Render novel views\n",
    "    \"\"\"\n",
    "    images, poses, camera_angle_x = load_synthetic_images_and_camera_metadata(data_dir)\n",
    "    N, H, W, _ = images.shape\n",
    "    focal = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Tested on laptop & oscar\n",
    "    print(f\"Using device: {device}\")\n",
    "    num_gaussians = 10000 # Honestly our most limiting factor\n",
    "    model = GaussianSplattingModel(num_gaussians=num_gaussians, device=device)\n",
    "    epochs = 500\n",
    "    model = train_gaussian_splatting(model, images, poses, camera_angle_x, epochs=epochs)\n",
    "    render_novel_views(model, output_dir, H, W, focal, num_frames=30)\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
