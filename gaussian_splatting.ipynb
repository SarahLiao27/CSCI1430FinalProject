{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262abce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch\n",
    "# %pip install opencv-python\n",
    "# %pip install numpy\n",
    "# %pip install tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synthetic_images_and_camera_metadata(class_dir, num_classes=8):\n",
    "    \"\"\"\n",
    "    Load images, poses, and camera angle from the synthetic dataset of choice.\n",
    "\n",
    "    Params:\n",
    "    - class_dir: Directory for the dataset    \n",
    "    Returns:\n",
    "    - images: Tensor of shape [N, H, W, 3]\n",
    "    - poses: Tensor of shape [N, 4, 4]\n",
    "    - camera_angle_x: Field of view in the x-direction\n",
    "    \"\"\"\n",
    "    # Gets the path\n",
    "    json_path = os.path.join(class_dir, \"transforms_train.json\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Loads images and poses into tensors\n",
    "    images, poses = [], []\n",
    "    for f in data['frames']:\n",
    "        img_file = os.path.join(class_dir, f['file_path'] + '.png')\n",
    "        raw = imageio.v2.imread(img_file)\n",
    "        if raw.ndim == 3 and raw.shape[2] == 4:\n",
    "            raw = raw[..., :3]\n",
    "        elif raw.ndim == 2:\n",
    "            raw = np.stack([raw, raw, raw], axis=-1)\n",
    "        img = raw.astype(np.float32) / 255.0\n",
    "        images.append(img)\n",
    "        poses.append(np.array(f['transform_matrix'], dtype=np.float32))\n",
    "    images = torch.tensor(np.stack(images), dtype=torch.float32)\n",
    "    poses = torch.tensor(np.stack(poses), dtype=torch.float32)\n",
    "    cam_ang = float(data['camera_angle_x'])\n",
    "    N, H, W, _ = images.shape\n",
    "    print(f\"Loaded dataset: images: {images.shape}, poses: {poses.shape}, camera_angle_x: {cam_ang:.4f}\")\n",
    "\n",
    "    # Plot example for comparsion\n",
    "    test_img = images[N-1]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(test_img.numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Example image\")\n",
    "    plt.show()\n",
    "    \n",
    "    return images, poses, cam_ang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829e002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSplattingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    3D Gaussian Splatting model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_gaussians=5000, device='cpu'):\n",
    "        super(GaussianSplattingModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_gaussians = num_gaussians\n",
    "        # Gaussian parameters: positions, scale, rotation, colors, opacity\n",
    "        # The xyz coordinates of each Gaussian\n",
    "        self.means = nn.Parameter(torch.randn(num_gaussians, 3, device=device) * 2.0)\n",
    "        # The scales controls the size of each Gaussian\n",
    "        self.scales = nn.Parameter(torch.ones(num_gaussians, 3, device=device) * 0.1)\n",
    "        # Rotation represented as quaternions for 3D rotation(common practice for most rendering models)\n",
    "        self.rotations = nn.Parameter(torch.zeros(num_gaussians, 4, device=device))\n",
    "        self.rotations.data[:, 0] = 1.0  # w component = 1 for identity quaternion\n",
    "        # Random RBG values for each Gaussian for now\n",
    "        self.colors = nn.Parameter(torch.rand(num_gaussians, 3, device=device))\n",
    "        # Start with small values for opacities so we can see through them initally\n",
    "        self.opacities = nn.Parameter(torch.ones(num_gaussians, 1, device=device) * 0.01)\n",
    "    \n",
    "    def quaternion_to_rotation_matrix(self, q):\n",
    "        \"\"\"Turns our quaternions to rotation matrices\"\"\"\n",
    "        q = F.normalize(q, dim=1)\n",
    "        w, x, y, z = q[:, 0], q[:, 1], q[:, 2], q[:, 3]\n",
    "        R = torch.zeros(q.shape[0], 3, 3, device=q.device)\n",
    "        # Standard quaternion to matrix formula\n",
    "        R[:, 0, 0] = 1 - 2 * (y**2 + z**2)\n",
    "        R[:, 0, 1] = 2 * (x * y - w * z)\n",
    "        R[:, 0, 2] = 2 * (x * z + w * y)\n",
    "        R[:, 1, 0] = 2 * (x * y + w * z)\n",
    "        R[:, 1, 1] = 1 - 2 * (x**2 + z**2)\n",
    "        R[:, 1, 2] = 2 * (y * z - w * x)\n",
    "        R[:, 2, 0] = 2 * (x * z - w * y)\n",
    "        R[:, 2, 1] = 2 * (y * z + w * x)\n",
    "        R[:, 2, 2] = 1 - 2 * (x**2 + y**2)\n",
    "        \n",
    "        return R\n",
    "    \n",
    "    def compute_covariance_matrices(self):\n",
    "        \"\"\"Compute covariance matrices for all Gaussians--what is used to \"splat\" each Gaussian into the image.\"\"\"\n",
    "        rotations = self.quaternion_to_rotation_matrix(self.rotations)\n",
    "        scales = torch.diag_embed(self.scales**2)\n",
    "        # For each Gaussian: rotation @ scale @ rotation.transpose or R * S * R^T\n",
    "        covariances = torch.bmm(torch.bmm(rotations, scales), rotations.transpose(1, 2))\n",
    "        \n",
    "        return covariances\n",
    "    \n",
    "    def project_gaussians_to_image(self, camera_pose, H, W, focal):\n",
    "        \"\"\"\n",
    "        Splat the 3D Gaussians onto the image plane.\n",
    "        \n",
    "        Params:\n",
    "        - camera_pose: [4, 4], the camera transformation matrix\n",
    "        - H, W: Image height and width\n",
    "        - focal: Focal length\n",
    "        Returns:\n",
    "        - centers_2d: [num_gaussians, 2], the 2D centers of our projected Gaussians\n",
    "        - covs_2d: [num_gaussians, 2, 2], the 2D covariance matrices of projections\n",
    "        - depths: [num_gaussians] the depths of Gaussians from camera\n",
    "        \"\"\"\n",
    "        # Transform Gaussian means into camera space\n",
    "        R = camera_pose[:3, :3] # Camera rotation\n",
    "        t = camera_pose[:3, 3] # Camera translation\n",
    "        means_camera = torch.matmul(self.means, R.t()) + t\n",
    "        depths = means_camera[:, 2]\n",
    "        valid_mask = depths > 0 # Only keep points in front\n",
    "\n",
    "        # Project 3D centers to 2D image coordinates\n",
    "        centers_2d = torch.zeros(self.num_gaussians, 2, device=self.device)\n",
    "        centers_2d[:, 0] = focal * means_camera[:, 0] / torch.clamp(means_camera[:, 2], min=1e-10) + W / 2\n",
    "        centers_2d[:, 1] = focal * means_camera[:, 1] / torch.clamp(means_camera[:, 2], min=1e-10) + H / 2\n",
    "\n",
    "        # Compute the 3D covariance matrices and project their covariance ellipses to 2D\n",
    "        covs_3d = self.compute_covariance_matrices()\n",
    "        covs_camera = torch.bmm(torch.bmm(R.expand(self.num_gaussians, 3, 3), covs_3d), R.expand(self.num_gaussians, 3, 3).transpose(1, 2))\n",
    "        J = torch.zeros(self.num_gaussians, 2, 3, device=self.device) # Jacobian matrix (helpful to maintain Gaussian's ellipsoidal shapes )\n",
    "        Z = torch.clamp(means_camera[:, 2], min=1e-10)\n",
    "        J[:, 0, 0] = focal / Z\n",
    "        J[:, 0, 2] = -focal * means_camera[:, 0] / (Z * Z)\n",
    "        J[:, 1, 1] = focal / Z\n",
    "        J[:, 1, 2] = -focal * means_camera[:, 1] / (Z * Z)\n",
    "        covs_2d = torch.bmm(torch.bmm(J, covs_camera), J.transpose(1, 2))\n",
    "        covs_2d = covs_2d + torch.eye(2, device=self.device).unsqueeze(0) * 1e-6 # To avoid division by 0\n",
    "        \n",
    "        return centers_2d, covs_2d, depths, valid_mask\n",
    "    \n",
    "    def render_image(self, camera_pose, H, W, focal, bg_color=None):\n",
    "        \"\"\"\n",
    "        Render an image by splatting all our Gaussians onto the image plane.\n",
    "        \n",
    "        Params:\n",
    "        - camera_pose: [4, 4], the camera transformation matrix\n",
    "        - H, W: Image height and width\n",
    "        - focal: Focal length\n",
    "        - bg_color: Background color(set to black)\n",
    "        Returns:\n",
    "        - rendered_image: [H, W, 3], rendered image\n",
    "        \"\"\"\n",
    "        if bg_color is None:\n",
    "            bg_color = torch.zeros(3, device=self.device) # Default black\n",
    "        else:\n",
    "            bg_color = torch.tensor(bg_color, device=self.device)\n",
    "        # Get each gaussian blob's 2d center, covariance, depth and a mask that tells us which are in front of the camera\n",
    "        centers_2d, covs_2d, depths, valid_mask = self.project_gaussians_to_image(camera_pose, H, W, focal)\n",
    "        rendered_image = torch.zeros(H, W, 3, device=self.device)\n",
    "        alpha_acc = torch.zeros(H, W, 1, device=self.device)\n",
    "        # Sort Gaussians by depth and remove those behind the camera\n",
    "        valid_indices = torch.where(valid_mask)[0]\n",
    "        if len(valid_indices) == 0:\n",
    "            return torch.ones(H, W, 3, device=self.device) * bg_color.unsqueeze(0).unsqueeze(0)\n",
    "        sorted_indices = valid_indices[torch.argsort(depths[valid_indices], descending=True)]\n",
    "        \n",
    "        # Iterate through sorted Gaussians to determine which pixels it affects and blend its color+opacity into the image.\n",
    "        for idx in sorted_indices:\n",
    "            center = centers_2d[idx]\n",
    "            cov = covs_2d[idx]\n",
    "            color = self.colors[idx]\n",
    "            opacity = torch.sigmoid(self.opacities[idx, 0])\n",
    "            # Skip Gaussians skip if far outside the image\n",
    "            if (center[0] < -W/2 or center[0] > W*1.5 or \n",
    "                center[1] < -H/2 or center[1] > H*1.5):\n",
    "                continue\n",
    "            # Use a bounding box for speed and efficiency(we really need it lol)\n",
    "            std_dev = torch.sqrt(torch.diagonal(cov) + 1e-10)\n",
    "            radius = 3 * torch.max(std_dev).item()\n",
    "            # Compute bounding box\n",
    "            min_x = max(0, int(center[0] - radius))\n",
    "            max_x = min(W, int(center[0] + radius + 1))\n",
    "            min_y = max(0, int(center[1] - radius))\n",
    "            max_y = min(H, int(center[1] + radius + 1))\n",
    "            if min_x >= max_x or min_y >= max_y: # Skip if bounding box is empty\n",
    "                continue\n",
    "            y_coords, x_coords = torch.meshgrid( torch.arange(min_y, max_y, device=self.device), torch.arange(min_x, max_x, device=self.device), indexing='ij')\n",
    "            box_coords = torch.stack([x_coords.flatten(), y_coords.flatten()], dim=1)\n",
    "            diff = box_coords - center.unsqueeze(0)\n",
    "            try: # we need this in the case the covariance doesn't have a inverse (singular) \n",
    "                inv_cov = torch.inverse(cov)\n",
    "            except:\n",
    "                inv_cov = torch.diag(1.0 / (torch.diagonal(cov) + 1e-10))\n",
    "            exponent = -0.5 * torch.sum(torch.matmul(diff, inv_cov) * diff, dim=1)\n",
    "            gauss_val = torch.exp(exponent) * opacity\n",
    "            gauss_val = gauss_val.reshape(max_y - min_y, max_x - min_x, 1) # For broadcasting\n",
    "            color_contribution = color.unsqueeze(0).unsqueeze(0) * gauss_val\n",
    "            # We use Alpha‑blending here it lets us layer semi‑transparent Gaussians correctly\n",
    "            current_alpha = 1.0 - alpha_acc[min_y:max_y, min_x:max_x]\n",
    "            alpha_acc[min_y:max_y, min_x:max_x] = alpha_acc[min_y:max_y, min_x:max_x] + gauss_val * current_alpha\n",
    "            rendered_image[min_y:max_y, min_x:max_x] = (rendered_image[min_y:max_y, min_x:max_x] + color_contribution * current_alpha)\n",
    "        # For pixels that may be partially or not covered at all we cover it with the background\n",
    "        rendered_image = rendered_image + bg_color.unsqueeze(0).unsqueeze(0) * (1.0 - alpha_acc)\n",
    "        \n",
    "        return rendered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec1cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera_pose_from_angle(theta, radius=4.0, height=2.0):\n",
    "    \"\"\"Create a camera pose matrix from an angle around the y-axis.\"\"\"\n",
    "    # Camera's world position\n",
    "    camera_position = np.array([radius * np.cos(theta), height, radius * np.sin(theta)])\n",
    "    # Aim the camera toward the origin\n",
    "    forward = -camera_position / np.linalg.norm(camera_position)\n",
    "    up = np.array([0, 1, 0]) # Initial \"up\" direction\n",
    "    # Camera's right vector \n",
    "    right = np.cross(up, forward)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    up = np.cross(forward, right)\n",
    "    rotation = np.stack([right, up, forward], axis=1)\n",
    "    # Construct 4x4 transformation matrix\n",
    "    pose = np.eye(4)\n",
    "    pose[:3, :3] = rotation\n",
    "    pose[:3, 3] = camera_position\n",
    "    \n",
    "    return torch.tensor(pose, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "3c4a8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gaussian_splatting(model, images, poses, camera_angle_x, epochs=100, lr=0.01, prune_threshold=0):\n",
    "    \"\"\"\n",
    "    Trains the Gaussian Splatting model.\n",
    "    \n",
    "    Params:\n",
    "    - model: GaussianSplattingModel\n",
    "    - images: [N, H, W, 3], the training images\n",
    "    - poses: [N, 4, 4], the camera poses\n",
    "    - camera_angle_x: Field of view in x-direction\n",
    "    - epochs: Number of training epochs\n",
    "    - lr: Learning rate\n",
    "    - prune_threshold: Opacity value below which Gaussians will be pruned\n",
    "    \"\"\"\n",
    "    N, H, W, _ = images.shape\n",
    "    focal = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.means, 'lr': lr},\n",
    "        {'params': model.scales, 'lr': lr},\n",
    "        {'params': model.rotations, 'lr': lr * 0.1},  # Lower learning rate for rotations\n",
    "        {'params': model.colors, 'lr': lr},\n",
    "        {'params': model.opacities, 'lr': lr * 0.1}  # Lower learning rate for opacities\n",
    "    ])\n",
    "    indices = torch.randperm(N)\n",
    "    losses = []  \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        indices = torch.randperm(N)\n",
    "        # Process each image\n",
    "        for i in tqdm(range(N), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            idx = indices[i].item()\n",
    "            target_image = images[idx].to(model.device)\n",
    "            pose = poses[idx].to(model.device)\n",
    "            rendered_image = model.render_image(pose, H, W, focal)\n",
    "            loss = F.mse_loss(rendered_image, target_image)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # To keep the scales positive\n",
    "            with torch.no_grad():\n",
    "                model.scales.data.clamp_(min=0.001, max=1.0)\n",
    "                model.rotations.data = F.normalize(model.rotations.data, dim=1) # Normalize quaternions\n",
    "                model.colors.data.clamp_(0, 1) # Clamp colors to [0, 1]\n",
    "                \n",
    "                if prune_threshold > 0 and (epoch + 1) % 10 == 0:\n",
    "                    # prune low-opacity Gaussians every 10 epochs\n",
    "                    low_opacity_mask = model.opacities.data.squeeze() < prune_threshold\n",
    "                    model.means.data[low_opacity_mask] = 0\n",
    "                    model.scales.data[low_opacity_mask] = 0\n",
    "                    model.rotations.data[low_opacity_mask] = 0\n",
    "                    model.colors.data[low_opacity_mask] = 0\n",
    "                    model.opacities.data[low_opacity_mask] = 0\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / N\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        # Show the splat progress every 5 epochs\n",
    "        save_dir = f\"generated_images{model.num_gaussians}g{epochs}e\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if (epoch + 1) % 1 == 0 or epoch == 0:\n",
    "            with torch.no_grad():\n",
    "                test_pose = poses[0].to(model.device)\n",
    "                test_render = model.render_image(test_pose, H, W, focal)\n",
    "                \n",
    "\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(images[0].cpu().numpy())\n",
    "                plt.title(\"Target Image\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.imshow(test_render.cpu().numpy())\n",
    "                plt.title(f\"Rendered Image (Epoch {epoch+1})\")\n",
    "                plt.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "\n",
    "                # Save the figure to the specified directory\n",
    "                save_path = os.path.join(save_dir, f\"epoch_{epoch+1}.png\")\n",
    "                plt.savefig(save_path)\n",
    "                plt.close()  # Close the plot to free memory1\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29bca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_novel_views(model, output_dir, H, W, focal, poses, num_frames=30):\n",
    "    \"\"\"\n",
    "    Render novel views by creating a spiral path around poses[0].\n",
    "\n",
    "    Params:\n",
    "    - model: Trained GaussianSplattingModel\n",
    "    - output_dir: Directory to save rendered images\n",
    "    - H, W: Image height and width\n",
    "    - focal: Focal length\n",
    "    - poses: [N, 4, 4] tensor of camera poses (torch.Tensor)\n",
    "    - num_frames: Number of novel views to render\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    base_pose = poses[0].to(model.device)  # Use the first pose as center\n",
    "\n",
    "    def create_spiral_poses_around(pose_center, radius=2.0, height=0.5, num_frames=30):\n",
    "        poses = []\n",
    "        for i in range(num_frames):\n",
    "            angle = 2 * math.pi * i / num_frames\n",
    "            # Spiral offset in the local frame\n",
    "            offset = torch.tensor([\n",
    "                radius * math.cos(angle),\n",
    "                radius * math.sin(angle),\n",
    "                height * math.sin(angle * 0.5)\n",
    "            ], device=pose_center.device)\n",
    "            # Transform to world coordinates\n",
    "            world_pos = pose_center[:3, 3] + pose_center[:3, :3] @ offset\n",
    "            # Look-at target is the original camera position (center of the object)\n",
    "            look_at = pose_center[:3, 3]\n",
    "            forward = (look_at - world_pos)\n",
    "            forward = forward / torch.norm(forward)\n",
    "            # Up vector from base pose\n",
    "            up = pose_center[:3, 1]\n",
    "            right = torch.cross(up, forward)\n",
    "            right = right / torch.norm(right)\n",
    "            up = torch.cross(forward, right)\n",
    "            R = torch.stack([right, up, forward], dim=1)\n",
    "            T = world_pos\n",
    "            pose = torch.eye(4, device=pose_center.device)\n",
    "            pose[:3, :3] = R\n",
    "            pose[:3, 3] = T\n",
    "            poses.append(pose)\n",
    "\n",
    "        return poses\n",
    "    \n",
    "    # Generate the spiral path\n",
    "    spiral_poses = create_spiral_poses_around(base_pose, radius=2.0, height=0.5, num_frames=num_frames)\n",
    "    with torch.no_grad():\n",
    "        for i, camera_pose in enumerate(spiral_poses):\n",
    "            rendered_image = model.render_image(camera_pose, H, W, focal)\n",
    "            img_np = rendered_image.cpu().numpy()\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            img_uint8 = (img_np * 255).astype(np.uint8)\n",
    "            imageio.imwrite(os.path.join(output_dir, f\"frame_{i:03d}.png\"), img_uint8)\n",
    "            # Optionally visualize every 5th frame\n",
    "            if i % 5 == 0:\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(img_np)\n",
    "                plt.title(f\"Frame {i}/{num_frames}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "    print(f\"Rendered {num_frames} novel views to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_video(train_dir=\"hotdog_final\", video_name=\"hotdog_angles.mp4\", fps=5):\n",
    "    \"\"\"\n",
    "    Creates an MP4 video using sequence of images\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_dir: Input path to images\n",
    "    video_name: Output file name\n",
    "    fps: Frames per second\n",
    "    \"\"\"\n",
    "    images = [img for img in sorted(os.listdir(train_dir)) if img.endswith(\".png\")]\n",
    "    first = cv2.imread(os.path.join(train_dir, images[0]))\n",
    "    H, W, _ = first.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, fps, (W, H))\n",
    "    if not video.isOpened():\n",
    "        raise RuntimeError(f\"Couldn’t open video writer for {video_name}\")\n",
    "    for img in images:\n",
    "        frame = cv2.imread(os.path.join(train_dir, img))\n",
    "        video.write(frame)\n",
    "    video.release()\n",
    "\n",
    "    print(f\"Saved {video_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize():\n",
    "    # Assume rays_o and rays_d from generate_camera_rays\n",
    "    rays_o, rays_d = generate_camera_rays(camera_pose, H=20, W=20)\n",
    "\n",
    "    # Downsample for visualization\n",
    "    subset = 50\n",
    "    rays_o_sub = rays_o[::subset]\n",
    "    rays_d_sub = rays_d[::subset]\n",
    "\n",
    "    # Plot in 2D (e.g., x-z plane)\n",
    "    plt.quiver(rays_o_sub[:, 0], rays_o_sub[:, 2], rays_d_sub[:, 0], rays_d_sub[:, 2])\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Z\")\n",
    "    plt.title(\"Ray directions in X-Z plane\")\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62f588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def export_gaussians_to_ply(model, filename=\"ply_output/output.ply\"):\n",
    "    \"\"\"\n",
    "    export the Gaussian parameters from the model into a .ply file.\n",
    "    can view the .ply in supersplat to visualize the result\n",
    "    \"\"\"\n",
    "    num_gaussians = model.num_gaussians\n",
    "    means = model.means.detach().cpu().numpy()\n",
    "    colors = model.colors.detach().cpu().numpy()\n",
    "    opacities = torch.sigmoid(model.opacities).detach().cpu().numpy()\n",
    "    scales = model.scales.detach().cpu().numpy()\n",
    "    rotations = model.rotations.detach().cpu().numpy()\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        header = f\"\"\"ply\n",
    "format binary_little_endian 1.0\n",
    "element vertex {num_gaussians}\n",
    "property float x\n",
    "property float y\n",
    "property float z\n",
    "property float f_dc_0\n",
    "property float f_dc_1\n",
    "property float f_dc_2\n",
    "property float opacity\n",
    "property float scale_0\n",
    "property float scale_1\n",
    "property float scale_2\n",
    "property float rot_0\n",
    "property float rot_1\n",
    "property float rot_2\n",
    "property float rot_3\n",
    "end_header\n",
    "\"\"\"\n",
    "        f.write(header.encode('utf-8'))\n",
    "\n",
    "        # write each Gaussian\n",
    "        for i in range(num_gaussians):\n",
    "            data = struct.pack(\n",
    "                '<fff'      # x, y, z\n",
    "                'fff'       # r, g, b (float format)\n",
    "                'f'         # opacity\n",
    "                'fff'       # scale_x, scale_y, scale_z\n",
    "                'ffff',     # quaternion (w, x, y, z)\n",
    "                *means[i],\n",
    "                *colors[i],\n",
    "                opacities[i][0],\n",
    "                *scales[i],\n",
    "                *rotations[i]\n",
    "            )\n",
    "            f.write(data)\n",
    "\n",
    "    print(f\"Wrote {num_gaussians} Gaussians to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b25108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(data_dir=\"data/nerf_synthetic/hotdog\", output_dir=\"gaussian_output\"):\n",
    "    \"\"\"\n",
    "    Main pipeline for Gaussian Splatting:\n",
    "    1. Load data\n",
    "    2. Create model\n",
    "    3. Train model\n",
    "    4. Render novel views\n",
    "    \"\"\"\n",
    "    images, poses, camera_angle_x = load_synthetic_images_and_camera_metadata(data_dir)\n",
    "    N, H, W, _ = images.shape\n",
    "    focal = 0.5 * W / np.tan(0.5 * camera_angle_x)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Tested on laptop & oscar\n",
    "    print(f\"Using device: {device}\")\n",
    "    num_gaussians = 2000 # Honestly our most limiting factor\n",
    "    model = GaussianSplattingModel(num_gaussians=num_gaussians, device=device)\n",
    "    epochs = 50\n",
    "    model = train_gaussian_splatting(model, images, poses, camera_angle_x, epochs=epochs)\n",
    "    # render_novel_views(model, output_dir, H, W, focal, num_frames=30)\n",
    "    # create_training_video()\n",
    "    export_gaussians_to_ply(model, f\"ply_output/hotdog{num_gaussians}g{epochs}e.ply\")\n",
    "\n",
    "    print(\"Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
